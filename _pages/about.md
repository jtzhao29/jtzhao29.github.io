---
permalink: /
# title: "Academic Pages is a ready-to-fork GitHub Pages template for academic personal websites"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am a third-year undergraduate majoring in Physics at Shanghai Jiaotong University, with a great interst in AI. 

See [Research interests](#research-interests) part for more information about what I am intersted, and see my [CV](/files/Jiatong Zhao_Resume_202601.pdf) for more details. If you share a similar passion with me, feel free to reach out and connect with me!

## news
<!-- 十一月的英文是？ -->

- **Jan,2026**: I am very glad to see our paper is accepted by ICLR 2026! [geobench:Rethinking Multimodal Geometric Problem-Solving via Hierarchical Evaluation](https://arxiv.org/abs/2512.24119)

- **Oct,2025**: I am very honered to receive Xiaomi Scholarship!

<!-- - **Nov,2025**: I am very honered to Secure a PhD position at the SII, Shanghai via autumn camp!  -->



## Education
- BSc in Physics, (Zhiyuan Honor Collage) Shanghai Jiaotong University, 2027 (Expected)

## Research interests

I mainly focus on how to enable models to emergently develop **generalized reasoning abilities at a sustainale scale with RL**. I believe that as the scale of systems increases, whether a model can emergently develop generalized reasoning abilities without relying on external supervision is a key step toward AGI. My research interests in this area include:

1. sclabel RL: 
  - Emergent Reasoning Ability: whether RL can maximize exploration and reasoning in models without relying on well-predefined structure or SFT. I also explore the potential for pure RL to drive the emergence of latent reasoning processes, particularly in context of CoT reasoning. 
  - RL stability and robustness: a critical issue of current RL framework is instability, particularly in phenomenon of train-inference mismatch. In case that the current AI Infra issue of mismatch cannot be addressed easily, I aim to uncover the underlying cause of such instability and develop reliable RL training recipe.
  - RL self-evolving: develop scalable RL frameworks that enable models to autonomously improve their reasoning and performance without constant external reward, pushing towards more adaptable and self-sustaining systems.

2. AI + formal verifier system: 
  - I am interested in building scalable solutions where AI systems can autonomously verify their own reasoning and results through interaction with a verifier system(**PLVR**)[^4], which might be a potential pathway to achieving **scalability**.
  - How to design task generation mechanisms that push the model to propose tasks that are tough but not too difficult. This touches on concepts like curriculum learning or **auto self-play**.

## Research experience

- Sep 2025 - Present: Research Intern @ [THU C3I](https://c3i.ee.tsinghua.edu.cn/). Supervised by [Ning Ding](https://www.stingning.cn/)

- July 2025 - Dec 2025: Research Intern @ Big AI Dream Lab, Shanghai AI Lab. Supervised by [Jie Fu](https://bigaidream.github.io/)

- March 2025 - May 2025: Research Intern Supervised by Prof.[Junchi Yan](https://thinklab.sjtu.edu.cn/) and Prof.[Renxiu Xia](https://scholar.google.com/citations?user=E520fqQAAAAJ&hl=zh-CN) @ SAI, Shanghai Jiaotong University
  - Hierarchical GeoBench: 	Propose a hierarchical benchmark featuring four reasoning levels in geometric problem-solving: Visual Perception, Goal-Oriented Planning, Rigorous Theorem Application, and Self-Reflective Backtracking. 
  - have been accepted by ICLR 2026!


- Augest 2024: Research Assistant in [Zhangjiang National Lab](https://www.zjlab.ac.cn/), where I completed an AI4PHY project on my own. 


<!-- 

[^1]: [SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training](https://arxiv.org/pdf/2501.17161)

[^2]: [From f(x) and g(x) to f(g(x)): LLMs Learn New Skills in RL by Composing Old Ones](https://husky-morocco-f72.notion.site/From-f-x-and-g-x-to-f-g-x-LLMs-Learn-New-Skills-in-RL-by-Composing-Old-Ones-2499aba4486f802c8108e76a12af3020)

[^3]: [Reasoning models don't always say what they think](https://www.anthropic.com/research/reasoning-models-dont-say-think)

[^4]: [Absolute Zero: Reinforced Self-play Reasoning with Zero Data](https://arxiv.org/abs/2505.03335) -->